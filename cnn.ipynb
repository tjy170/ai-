{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f1cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f30d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 定义标签和类别字典以及常量 N\n",
    "label_dict = {\"automobile\": 0, \"airplane\": 1, \"frog\": 2}\n",
    "class_dict = {0: \"automobile\", 1: \"airplane\", 2: \"frog\"}\n",
    "N = 3  # 总共三个类别\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, _data_dir, _transform, _loader):\n",
    "        # 获取标签文件夹列表\n",
    "        self.labels = [_label for _label in os.listdir(_data_dir)]\n",
    "        _file_path_label_list = [(os.path.join(_data_dir, _label, _img_fn), _label)\n",
    "                                 for _label in os.listdir(_data_dir)\n",
    "                                 for _img_fn in os.listdir(os.path.join(_data_dir, _label))\n",
    "                                 if not os.path.isdir(os.path.join(_data_dir, _label, _img_fn))]\n",
    "\n",
    "        self.data = [(_loader(_fp), label_dict[_label]) for _fp, _label in _file_path_label_list]\n",
    "        self.transform = _transform\n",
    "#         self.labels = [label for label in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, label))]\n",
    "        \n",
    "#         # 获取图像路径和标签的配对列表\n",
    "#         self.data = []\n",
    "#         for label in self.labels:\n",
    "#             label_path = os.path.join(data_dir, label)\n",
    "#             for img_fn in os.listdir(label_path):\n",
    "#                 img_path = os.path.join(label_path, img_fn)\n",
    "#                 if not os.path.isdir(img_path):\n",
    "#                     self.data.append((img_path, label_dict[label]))\n",
    "\n",
    "        # self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        _img, _label = self.data[item]\n",
    "        _img = self.transform(_img)\n",
    "        return _img, _label\n",
    "        # img_path, label = self.data[index]\n",
    "        # img = Image.open(img_path).convert('RGB')\n",
    "        # if self.transform:\n",
    "        #     img = self.transform(img)\n",
    "        # return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def load_data():\n",
    "    print('data processing...')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # normalization\n",
    "    ])\n",
    "    data_dir = \"data/training_data/\"\n",
    "    train_dataset = MyDataset(data_dir, transform, _loader=lambda _path: Image.open(_path).convert('RGB'))\n",
    "    test_dataset = MyDataset(data_dir, transform, _loader=lambda _path: Image.open(_path).convert('RGB'))\n",
    "\n",
    "    train_size = int(len(train_dataset) * 0.8)\n",
    "    validate_size = len(train_dataset) - train_size\n",
    "    train, val = torch.utils.data.random_split(train_dataset, [train_size, validate_size])\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train, batch_size=50, shuffle=True, num_workers=0)\n",
    "    val_data_loader = DataLoader(dataset=val, batch_size=50, shuffle=True, num_workers=0)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=50, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_data_loader, val_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f25d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from classify.data_process import load_data, N, label_dict, class_dict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from classify.decorator import metric_time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(20)\n",
    "\n",
    "\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3 * 3 * 64, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.out = nn.Linear(10, N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.out(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_val_loss(model, Val):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    val_loss = []\n",
    "    for (data, target) in Val:\n",
    "        data, target = data.to(device), target.long().to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss.append(loss.cpu().item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "@metric_time\n",
    "def train():\n",
    "    train_data_loader, val_data_loader, _ = load_data()\n",
    "    print('train...')\n",
    "    epoch_num = 30\n",
    "    best_model = None\n",
    "    min_epochs = 5\n",
    "    min_val_loss = 5\n",
    "    model = cnn().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    for epoch in tqdm(range(epoch_num), ascii=True):\n",
    "        train_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
    "            data, target = data.to(device), target.long().to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.cpu().item())\n",
    "        # validation\n",
    "        val_loss = get_val_loss(model, val_data_loader)\n",
    "        if epoch + 1 > min_epochs and val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        tqdm.write('Epoch {:03d} train_loss {:.5f} val_loss {:.5f}'.format(epoch, np.mean(train_loss), val_loss))\n",
    "\n",
    "    torch.save(best_model.state_dict(), \"model/cnn.pkl\")\n",
    "\n",
    "\n",
    "@metric_time\n",
    "def test():\n",
    "    _, _, test_dataset = load_data()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = cnn().to(device)\n",
    "    model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
    "    total = 0\n",
    "    current = 0\n",
    "    model.eval()\n",
    "    for (data, target) in test_dataset:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = torch.max(outputs.data, 1)[1].data\n",
    "        total += target.size(0)\n",
    "        current += (predicted == target).sum()\n",
    "\n",
    "    print('Accuracy:%d%%' % (100 * current / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "942df1cd",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cc888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n",
      "train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|3         | 1/30 [00:01<00:48,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss 1.08649 val_loss 1.09802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|6         | 2/30 [00:03<00:46,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train_loss 1.04207 val_loss 1.08652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|#         | 3/30 [00:04<00:44,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 train_loss 1.00149 val_loss 1.06026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|#3        | 4/30 [00:06<00:42,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 train_loss 0.95533 val_loss 1.00820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|#6        | 5/30 [00:08<00:40,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 train_loss 0.92095 val_loss 0.95487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|##        | 6/30 [00:09<00:38,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 train_loss 0.88718 val_loss 0.89527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|##3       | 7/30 [00:11<00:36,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 train_loss 0.87367 val_loss 0.89391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|##6       | 8/30 [00:12<00:34,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 train_loss 0.83389 val_loss 0.85359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|###       | 9/30 [00:14<00:33,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 train_loss 0.80426 val_loss 0.79433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|###3      | 10/30 [00:16<00:31,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 train_loss 0.78704 val_loss 0.83964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|###6      | 11/30 [00:17<00:30,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 train_loss 0.77802 val_loss 0.77653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|####      | 12/30 [00:19<00:29,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 train_loss 0.73820 val_loss 0.74557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|####3     | 13/30 [00:20<00:27,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 train_loss 0.73715 val_loss 0.71680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|####6     | 14/30 [00:22<00:25,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 train_loss 0.72496 val_loss 0.72251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|#####     | 15/30 [00:24<00:24,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 train_loss 0.70935 val_loss 0.73235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|#####3    | 16/30 [00:25<00:22,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 train_loss 0.70679 val_loss 0.71797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|#####6    | 17/30 [00:27<00:20,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 train_loss 0.70958 val_loss 0.79486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|######    | 18/30 [00:28<00:19,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 train_loss 0.68348 val_loss 0.70528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|######3   | 19/30 [00:30<00:17,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 train_loss 0.69646 val_loss 0.69554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|######6   | 20/30 [00:32<00:16,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 train_loss 0.68789 val_loss 0.72207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|#######   | 21/30 [00:33<00:14,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 train_loss 0.67558 val_loss 0.69623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|#######3  | 22/30 [00:35<00:12,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 train_loss 0.68496 val_loss 0.71201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|#######6  | 23/30 [00:36<00:11,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 train_loss 0.67031 val_loss 0.69586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|########  | 24/30 [00:38<00:09,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 train_loss 0.66027 val_loss 0.68786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|########3 | 25/30 [00:40<00:07,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 train_loss 0.67439 val_loss 0.70738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|########6 | 26/30 [00:41<00:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 train_loss 0.64709 val_loss 0.66755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|######### | 27/30 [00:43<00:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 train_loss 0.64382 val_loss 0.69572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|#########3| 28/30 [00:44<00:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 train_loss 0.64797 val_loss 0.74035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|#########6| 29/30 [00:46<00:01,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 train_loss 0.64784 val_loss 0.69221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 30/30 [00:48<00:00,  1.60s/it]\n",
      "\u001b[32m2024-11-29 10:17:16.877\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mclassify.decorator\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mtrain运行时间: 48.557636976242065 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 train_loss 0.65993 val_loss 0.66561\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e002d",
   "metadata": {},
   "source": [
    "# 测试模型预测准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ceb88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_13228/934605258.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
      "\u001b[32m2024-11-29 10:17:47.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mclassify.decorator\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mtest运行时间: 1.4184556007385254 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:91%\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d7ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1fee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_13228/3990064704.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n"
     ]
    }
   ],
   "source": [
    "model = cnn().to(device)\n",
    "model.load_state_dict(torch.load(\"model/cnn.pkl\"), False)\n",
    "model.eval()\n",
    "\n",
    "_img_path = \"data/testing_data/airplane/airplane-00029.jpg\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # normalization\n",
    "])\n",
    "img = Image.open(_img_path).convert('RGB')\n",
    "# 模拟批样本\n",
    "img_transform = transform(img).unsqueeze(0)\n",
    "\n",
    "output = model(img_transform)\n",
    "pred = class_dict[torch.max(output.data, 1)[1].data.item()]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f873ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2604d5-5150-414b-9ee4-09d55aaba8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
